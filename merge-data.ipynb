{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(data_folder=\"data\", main_file=\"li_jobs.json\"):\n",
    "    \"\"\"\n",
    "    Merges all JSON files from subfolders in the data folder into the main li_jobs.json file.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_folder (str): The root folder containing the data (default: 'data').\n",
    "    - main_file (str): The name of the main JSON file to update (default: 'li_jobs.json').\n",
    "    \"\"\"\n",
    "    main_file_path = os.path.join(data_folder, main_file)\n",
    "    \n",
    "    print(main_file_path)\n",
    "    \n",
    "    main_data = []\n",
    "    \n",
    "    # Walk through the data folder and its subfolders\n",
    "    for root, _, files in os.walk(data_folder):\n",
    "        for file in files:\n",
    "            # Process JSON files \n",
    "            if file.endswith(\".json\") and file != main_file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                \n",
    "                # Load the JSON file\n",
    "                try:\n",
    "                    sub_df = pd.read_json(file_path)\n",
    "                    sub_data = sub_df.to_dict(orient=\"records\")\n",
    "                    # Append the data to the main list\n",
    "                    main_data.extend(sub_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    # Convert back to DataFrame to handle any potential formatting\n",
    "    merged_df = pd.DataFrame(main_data)\n",
    "    \n",
    "    # Save the merged data back to the main file\n",
    "    merged_df.to_json(main_file_path, orient=\"records\", indent=2, force_ascii=False)\n",
    "    print(f\"Merged data saved to {main_file_path}. Total records: {len(merged_df)}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_by_poster_id(data_folder=\"data\", main_file=\"li_jobs.json\"):\n",
    "    \"\"\"\n",
    "    Removes duplicate entries in li_jobs.json based on the 'Poster Id' column.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_folder (str): The root folder containing the data (default: 'data').\n",
    "    - main_file (str): The name of the main JSON file to clean (default: 'li_jobs.json').\n",
    "    \"\"\"\n",
    "    main_file_path = os.path.join(data_folder, main_file)\n",
    "    \n",
    "    # Check if the main file exists\n",
    "    if not os.path.exists(main_file_path):\n",
    "        print(f\"Main file {main_file_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Load the main file\n",
    "    df = pd.read_json(main_file_path)\n",
    "    initial_count = len(df)\n",
    "    print(f\"Initial number of records: {initial_count}\")\n",
    "    \n",
    "    # Remove duplicates based on 'Poster Id', keeping the first occurrence\n",
    "    df_cleaned = df.drop_duplicates(subset=\"Poster Id\", keep=\"first\")\n",
    "    final_count = len(df_cleaned)\n",
    "    print(f\"Number of records after removing duplicates: {final_count}\")\n",
    "    print(f\"Removed {initial_count - final_count} duplicate records.\")\n",
    "    \n",
    "    # Save the cleaned data back to the main file\n",
    "    df_cleaned.to_json(main_file_path, orient=\"records\", indent=2)\n",
    "    print(f\"Cleaned data saved to {main_file_path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\li_jobs.json\n",
      "Processing file: data\\04-03\\jobsDetails_20250304.json\n",
      "Processing file: data\\31-03\\jobsDetails_20250331200718.json\n",
      "Processing file: data\\31-03\\jobsDetails_20250331202803.json\n",
      "Processing file: data\\31-03\\jobsDetails_20250331203727.json\n",
      "Processing file: data\\31-03\\jobsDetails_20250331204209.json\n",
      "Processing file: data\\31-03\\jobsDetails_20250331205341.json\n",
      "Processing file: data\\31-03\\jobsDetails_20250331210404.json\n",
      "Processing file: data\\31-03\\jobsDetails_20250331211440.json\n",
      "Merged data saved to data\\li_jobs.json. Total records: 6547\n"
     ]
    }
   ],
   "source": [
    "merge_json_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of records: 6547\n",
      "Number of records after removing duplicates: 2086\n",
      "Removed 4461 duplicate records.\n",
      "Cleaned data saved to data\\li_jobs.json\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates_by_poster_id()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
